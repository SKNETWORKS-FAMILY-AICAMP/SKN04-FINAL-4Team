import os
from typing import List
from langgraph.graph import StateGraph
from langchain_huggingface import HuggingFaceEndpoint
from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline
from langchain_community.llms import Llamafile
from transformers import AutoModelForCausalLM, AutoTokenizer
from langchain_community.llms import Ollama
from langchain_ollama.llms import OllamaLLM
from langchain_ollama import ChatOllama
from langgraph.graph import StateGraph, START, END
from langchain_core.tools import tool
from typing import TypedDict, Annotated
from langgraph.graph.message import add_messages
from langchain_core.messages import AIMessageChunk, HumanMessage
from langchain_teddynote.graphs import visualize_graph
from langgraph.prebuilt import ToolNode, tools_condition
from langchain_teddynote.messages import stream_graph, random_uuid
from typing import Literal
from langchain_core.runnables import RunnableConfig
from langchain_teddynote.tools.tavily import TavilySearch
from langchain_community.retrievers import TavilySearchAPIRetriever
from langchain_openai import ChatOpenAI
from langchain_chroma import Chroma
from langchain.embeddings import SentenceTransformerEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain.retrievers import BM25Retriever
from chromadb.utils import embedding_functions
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.llms import HuggingFaceHub
from langchain_huggingface import HuggingFaceEndpoint
from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from dotenv import load_dotenv
from langchain_teddynote import logging
from langgraph.checkpoint.memory import MemorySaver
from langchain_openai import OpenAIEmbeddings
from langgraph_utill import *
from functools import partial
import chromadb
import pandas as pd
from tqdm import tqdm
from bs4 import BeautifulSoup
from langchain_core.documents import Document
from langchain_chroma import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings


load_dotenv('./.env',verbose=True)

# í”„ë¡œì íŠ¸ ì´ë¦„ì„ ì…ë ¥í•©ë‹ˆë‹¤.
logging.langsmith("qa_langgraph")

chroma_client = chromadb.HttpClient(host='44.223.22.232', port=8000)
em = OpenAIEmbeddings(model="text-embedding-3-small")
law_db = Chroma(collection_name='law', client=chroma_client,embedding_function=em)

law_retriever = law_db.as_retriever(
    search_kwargs={"k": 4},  # ìƒìœ„ 3ê°œ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜´
)

documents = [Document(page_content=doc) for doc in law_db._collection.get().get("documents")]

bm25_retriever = BM25Retriever.from_documents(documents)
bm25_retriever.k = 3  # BM25ì—ì„œ ìƒìœ„ 3ê°œ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜´

law_retriever = law_retriever
manual_retriever = ...


final_prompt = """
Context:
{context}

User question: {question}

Answer:
"""

final_llm = ChatOllama(model="cqmodel:latest", num_thread=12, top_k=5) 

memory = MemorySaver()
graph_builder = StateGraph(State)

# ë…¸ë“œ ë“±ë¡ (Rewrite -> Domain -> Retrieve -> LLM -> END)

graph_builder.add_node(
    "RewriteQuery",
    partial(query_rewrite_llm)
)

graph_builder.add_node(
    "RouteDomain",
    partial(route_domain_llm)
)

graph_builder.add_node(
    "RetrieveLaw",
    partial(retrieve_document_law, law_retriever=law_retriever, bm25_retriever=bm25_retriever)
)

graph_builder.add_node(
    "RetrieveManual",
    partial(retrieve_document_manual, manual_retriever=manual_retriever, bm25_retriever=bm25_retriever)
)

graph_builder.add_node(
    "LLM",
    partial(call_model, prompt=final_prompt, llm=final_llm)
)

graph_builder.add_edge("RewriteQuery", "RouteDomain")

graph_builder.add_conditional_edges(
    "RouteDomain",
    domain_condition,
    {
        "law": "RetrieveLaw",
        "manual": "RetrieveManual"
    }
)

graph_builder.add_edge("RetrieveLaw", "LLM")
graph_builder.add_edge("RetrieveManual", "LLM")
graph_builder.add_edge("LLM", END)

graph_builder.set_entry_point("RewriteQuery")
graph = graph_builder.compile(checkpointer=memory)

config = RunnableConfig(
    recursion_limit=10,  # ìµœëŒ€ 10ê°œì˜ ë…¸ë“œê¹Œì§€ ë°©ë¬¸. ê·¸ ì´ìƒì€ RecursionError ë°œìƒ
    configurable={"thread_id": "6344"},  # ìŠ¤ë ˆë“œ ID ì„¤ì •
)

inputs = State(question="ê´€ì„¸ë²• ì œ89ì¡°ëŠ” ì–´ë–¤ ë²•ì´ì•¼?")

prev_node = ""
node_names = ["RewriteQuery", "RouteDomain", "RetrieveLaw", "RetrieveManual", "LLM"]
for chunk_msg, metadata in graph.stream(inputs, config, stream_mode="messages"):
    curr_node = metadata["langgraph_node"]
    # print(curr_node)
    # node_namesê°€ ë¹„ì–´ìˆê±°ë‚˜ í˜„ì¬ ë…¸ë“œê°€ node_namesì— ìˆëŠ” ê²½ìš°ì—ë§Œ ì²˜ë¦¬
    
    if not node_names or curr_node in node_names:
        # ì½œë°± í•¨ìˆ˜ê°€ ìˆëŠ” ê²½ìš° ì‹¤í–‰
        # if callback:
        #     callback({"node": curr_node, "content": chunk_msg.content})
        # ì½œë°±ì´ ì—†ëŠ” ê²½ìš° ê¸°ë³¸ ì¶œë ¥
        
        # if curr_node == "RewriteQuery":
        #     print(chunk_msg.content, end="")

            # ë…¸ë“œê°€ ë³€ê²½ëœ ê²½ìš°ì—ë§Œ êµ¬ë¶„ì„  ì¶œë ¥
        if curr_node != prev_node:
            print("\n" + "=" * 50)
            print(f"ğŸ”„ Node: \033[1;36m{curr_node}\033[0m ğŸ”„")
            print("- " * 25)
        print(chunk_msg.content, end="", flush=True)

        prev_node = curr_node