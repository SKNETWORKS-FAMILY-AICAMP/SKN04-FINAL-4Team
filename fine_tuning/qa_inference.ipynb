{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "788bf1a0-cb7e-458d-8549-a463ba230044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import huggingface_hub\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "from trl.core import LengthSampler\n",
    "from trl import (\n",
    "    PPOTrainer,\n",
    "    PPOConfig,\n",
    "    AutoModelForCausalLMWithValueHead,\n",
    "    create_reference_model,\n",
    "    DPOConfig,\n",
    "    DPOTrainer,\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    AutoPeftModel,\n",
    "    AutoPeftModelForCausalLM,\n",
    "    PeftModel,\n",
    "    LoraConfig,\n",
    "    LoftQConfig,\n",
    "    TaskType,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import (\n",
    "    get_chat_template,\n",
    "    train_on_responses_only,\n",
    "    standardize_sharegpt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46ebc915-1fb4-49e9-bb73-c8a67b841f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3062d21-9ac3-46a5-ad7e-62a3fb0fd3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.9: Fast Llama patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe. Max memory: 79.254 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.12.9 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'HoJL/qa_v3'\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,\n",
    "    max_seq_length=2048\n",
    ")\n",
    "model = model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e64149d3-43bd-4257-9790-30d69ce4d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('HoJL/context_qa_set', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af4575dc-60f1-4c5a-a4db-a830329d8edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.train_test_split(test_size=0.1)\n",
    "train_test = data['train'].train_test_split(test_size=0.1)\n",
    "dataset = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'valid': data['test'],\n",
    "    'test': train_test['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b87bb362-63c6-4359-985c-b9af8864b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "\n",
    "class StopOnToken(StoppingCriteria):\n",
    "    def __init__(self, stop_token_id):\n",
    "        self.stop_token_id = stop_token_id  # 정지 토큰 ID를 초기화합니다.\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        return (\n",
    "            self.stop_token_id in input_ids[0]\n",
    "        )  # 입력된 ID 중 정지 토큰 ID가 있으면 정지합니다.\n",
    "\n",
    "\n",
    "# end_token을 설정\n",
    "stop_token = \"<|end_of_text|>\"  # end_token으로 사용할 토큰을 설정합니다.\n",
    "stop_token_id = tokenizer.encode(stop_token, add_special_tokens=False)[\n",
    "    0\n",
    "]\n",
    "\n",
    "# Stopping criteria 설정\n",
    "stopping_criteria = StoppingCriteriaList(\n",
    "    [StopOnToken(stop_token_id)]\n",
    ")  # 정지 조건을 설정합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4c959b4-4cd2-46a7-bd65-d8c209dcdedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_prompt = \"\"\"\n",
    "너는 내용을 보고 질문에 맞는 답을 해주는 역할이야.\n",
    "\n",
    "###내용:\n",
    "{}\n",
    "###질문:\n",
    "{}\n",
    "###답:\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98c3170c-9098-497a-963e-e59b5ac50437",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "        inference_prompt.format(\n",
    "            row.context,\n",
    "            row.question,  # 지시사항\n",
    "            \"\",  # 출력 - 생성을 위해 이 부분을 비워둡니다!\n",
    "        ) for row in dataset['test'].to_pandas().itertuples()\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f162f67d-a8fe-4ac7-91e1-6dfe8bef38d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = dataset['test'][3]['context']\n",
    "question = dataset['test'][3]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "030ee5ff-5b94-4a3e-aa96-f5aba4db5178",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [inference_prompt.format(\n",
    "            context,\n",
    "            question,  # 지시사항\n",
    "            \"\",  # 출력 - 생성을 위해 이 부분을 비워둡니다!\n",
    "        )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1b0cdf6-231e-4b3c-8d58-a13674307224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "# FastLanguageModel을 이용하여 추론 속도를 2배 빠르게 설정합니다.\n",
    "print(tokenizer.padding_side)\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer(\n",
    "    texts[4],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d7a0932-8b63-4f57-800b-7cfe3879d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in inputs:\n",
    "    if isinstance(inputs[key], torch.Tensor):\n",
    "        inputs[key] = torch.nan_to_num(inputs[key], nan=0.0, posinf=1.0, neginf=-1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9792ec2-250a-4782-99a0-89880c77722d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs.get('input_ids')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a0a8b2-629f-4cbd-a369-b6248e13bafc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8f016d9-4170-4925-bd1c-7653e8b1c695",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        # streamer=text_streamer,\n",
    "        max_new_tokens=2048,  # 최대 생성 토큰 수를 설정합니다.\n",
    "        # stopping_criteria=stopping_criteria,  # 생성을 멈출 기준을 설정합니다.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ffb6e6f-1b22-46c8-88cf-fc5cb1583fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4596c611-48e9-4190-9890-9618018702eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n너는 내용을 보고 질문에 맞는 답을 해주는 역할이야.\\n\\n###내용:\\npassage 1: 문의하기. Dove® 웹사이트를 방문해 주셔서 감사합니다. 실시간 상담원과 통화하시려면 월요일부터 금요일까지 동부 시간 기준 오전 8:30부터 오후 6:00까지 1-800-761-DOVE (3683)로 전화해 주십시오. 의료 또는 제품 안전 비상 상황인 경우, 1-800-297-7421로 전화해 주십시오.\\n\\npassage 2: Dove® 웹사이트를 방문해 주셔서 감사합니다. 실시간 상담원과 통화하시려면 월요일부터 금요일까지 동부 시간 기준 오전 8:30부터 오후 6:00까지 1-800-761-DOVE (3683)로 전화해 주십시오. 의료 또는 제품 안전 비상 상황인 경우, 1-800-297-7421로 전화해 주십시오. 이 번호는 하루 24시간, 주 7일 이용 가능합니다.\\n\\npassage 3: 공유하기. Dove는 아름다움이란 최상의 상태를 느끼고 보이는 것이 적절한 관리의 결과라고 믿습니다. Dove는 항상 피부나 머리카락의 상태를 눈에 띄게 개선하고, 관리의 즐거운 경험을 제공하는 제품을 제공하는 것을 목표로 합니다. 왜냐하면 아름답게 보이고 느낄 때 더 행복해지기 때문입니다.\\n###질문:\\n도브 비누 본사는 어디에 있나요?\\n###답:\\n\\n주어진 문구를 바탕으로, 도브 비누 본사는 다음과 같은 정보를 제공합니다:\\n\\n* 도브 비누 본사는 도브 웹사이트를 방문해 주셔서 감사합니다 (문구 1). 도브 웹사이트를 방문하여 도브 비누의 제품과 서비스에 대한 정보를 찾으세요.\\n* 도브 비누 실시간 상담원은 월요일부터 금요일까지 동부 시간 기준 오전 8:30부터 오후 6:00까지 1-800-761-DOVE(3683)로 전화하실 수 있습니다 (문구 1). 도브 비누 실시간 상담원은 도브 비누의 제품과 서비스에 대한 질문이나 문제를 해결하실 수 있습니다.\\n* 도브 비누의 제품 안전 비상 상황은 1-800-297-7421로 전화하실 수 있습니다 (문구 2). 도브 비누의 제품 안전 비상 상황은 도브 비누의 제품에 대한 질문이나 문제를 해결하실 수 있습니다.\\n* 도브 비누의 제품은 아름다움을 향상시키고 피부나 머리카락의 상태를 개선하는 제품을 제공합니다 (문구 3). 도브 비누의 제품은 도브 비누의 제품과 서비스에 대한 정보를 찾으실 수 있습니다.\\n\\n따라서, 도브 비누 본사는 문구 1과 문구 2에서 언급된 도브 비누 웹사이트 또는 도브 비누 실시간 상담원으로 확인할 수 있습니다.\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b51211-7129-49b1-9461-6944e0a81196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
