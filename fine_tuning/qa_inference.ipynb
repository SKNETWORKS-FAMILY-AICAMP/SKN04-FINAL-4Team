{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "788bf1a0-cb7e-458d-8549-a463ba230044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import huggingface_hub\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "from trl.core import LengthSampler\n",
    "from trl import (\n",
    "    PPOTrainer,\n",
    "    PPOConfig,\n",
    "    AutoModelForCausalLMWithValueHead,\n",
    "    create_reference_model,\n",
    "    DPOConfig,\n",
    "    DPOTrainer,\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    AutoPeftModel,\n",
    "    AutoPeftModelForCausalLM,\n",
    "    PeftModel,\n",
    "    LoraConfig,\n",
    "    LoftQConfig,\n",
    "    TaskType,\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "from unsloth import is_bfloat16_supported\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import (\n",
    "    get_chat_template,\n",
    "    train_on_responses_only,\n",
    "    standardize_sharegpt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46ebc915-1fb4-49e9-bb73-c8a67b841f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3062d21-9ac3-46a5-ad7e-62a3fb0fd3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.9: Fast Llama patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe. Max memory: 79.254 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2024.12.9 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model_name = 'HoJL/qa_v3'\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    load_in_4bit=True,\n",
    "    dtype=None,\n",
    "    max_seq_length=2048\n",
    ")\n",
    "model = model.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e64149d3-43bd-4257-9790-30d69ce4d4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('HoJL/context_qa_set', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af4575dc-60f1-4c5a-a4db-a830329d8edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.train_test_split(test_size=0.1)\n",
    "train_test = data['train'].train_test_split(test_size=0.1)\n",
    "dataset = DatasetDict({\n",
    "    'train': train_test['train'],\n",
    "    'valid': data['test'],\n",
    "    'test': train_test['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b87bb362-63c6-4359-985c-b9af8864b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "\n",
    "class StopOnToken(StoppingCriteria):\n",
    "    def __init__(self, stop_token_id):\n",
    "        self.stop_token_id = stop_token_id  # ì •ì§€ í† í° IDë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        return (\n",
    "            self.stop_token_id in input_ids[0]\n",
    "        )  # ì…ë ¥ëœ ID ì¤‘ ì •ì§€ í† í° IDê°€ ìˆìœ¼ë©´ ì •ì§€í•©ë‹ˆë‹¤.\n",
    "\n",
    "\n",
    "# end_tokenì„ ì„¤ì •\n",
    "stop_token = \"<|end_of_text|>\"  # end_tokenìœ¼ë¡œ ì‚¬ìš©í•  í† í°ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "stop_token_id = tokenizer.encode(stop_token, add_special_tokens=False)[\n",
    "    0\n",
    "]\n",
    "\n",
    "# Stopping criteria ì„¤ì •\n",
    "stopping_criteria = StoppingCriteriaList(\n",
    "    [StopOnToken(stop_token_id)]\n",
    ")  # ì •ì§€ ì¡°ê±´ì„ ì„¤ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4c959b4-4cd2-46a7-bd65-d8c209dcdedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_prompt = \"\"\"\n",
    "ë„ˆëŠ” ë‚´ìš©ì„ ë³´ê³  ì§ˆë¬¸ì— ë§ëŠ” ë‹µì„ í•´ì£¼ëŠ” ì—­í• ì´ì•¼.\n",
    "\n",
    "###ë‚´ìš©:\n",
    "{}\n",
    "###ì§ˆë¬¸:\n",
    "{}\n",
    "###ë‹µ:\n",
    "{}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98c3170c-9098-497a-963e-e59b5ac50437",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "        inference_prompt.format(\n",
    "            row.context,\n",
    "            row.question,  # ì§€ì‹œì‚¬í•­\n",
    "            \"\",  # ì¶œë ¥ - ìƒì„±ì„ ìœ„í•´ ì´ ë¶€ë¶„ì„ ë¹„ì›Œë‘¡ë‹ˆë‹¤!\n",
    "        ) for row in dataset['test'].to_pandas().itertuples()\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f162f67d-a8fe-4ac7-91e1-6dfe8bef38d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = dataset['test'][3]['context']\n",
    "question = dataset['test'][3]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "030ee5ff-5b94-4a3e-aa96-f5aba4db5178",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [inference_prompt.format(\n",
    "            context,\n",
    "            question,  # ì§€ì‹œì‚¬í•­\n",
    "            \"\",  # ì¶œë ¥ - ìƒì„±ì„ ìœ„í•´ ì´ ë¶€ë¶„ì„ ë¹„ì›Œë‘¡ë‹ˆë‹¤!\n",
    "        )]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b1b0cdf6-231e-4b3c-8d58-a13674307224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "# FastLanguageModelì„ ì´ìš©í•˜ì—¬ ì¶”ë¡  ì†ë„ë¥¼ 2ë°° ë¹ ë¥´ê²Œ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "print(tokenizer.padding_side)\n",
    "FastLanguageModel.for_inference(model)\n",
    "inputs = tokenizer(\n",
    "    texts[4],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True\n",
    ").to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6d7a0932-8b63-4f57-800b-7cfe3879d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in inputs:\n",
    "    if isinstance(inputs[key], torch.Tensor):\n",
    "        inputs[key] = torch.nan_to_num(inputs[key], nan=0.0, posinf=1.0, neginf=-1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a9792ec2-250a-4782-99a0-89880c77722d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "350"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inputs.get('input_ids')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a0a8b2-629f-4cbd-a369-b6248e13bafc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e8f016d9-4170-4925-bd1c-7653e8b1c695",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_streamer = TextStreamer(tokenizer)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model.generate(\n",
    "        **inputs,\n",
    "        # streamer=text_streamer,\n",
    "        max_new_tokens=2048,  # ìµœëŒ€ ìƒì„± í† í° ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "        # stopping_criteria=stopping_criteria,  # ìƒì„±ì„ ë©ˆì¶œ ê¸°ì¤€ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3ffb6e6f-1b22-46c8-88cf-fc5cb1583fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4596c611-48e9-4190-9890-9618018702eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\në„ˆëŠ” ë‚´ìš©ì„ ë³´ê³  ì§ˆë¬¸ì— ë§ëŠ” ë‹µì„ í•´ì£¼ëŠ” ì—­í• ì´ì•¼.\\n\\n###ë‚´ìš©:\\npassage 1: ë¬¸ì˜í•˜ê¸°. DoveÂ® ì›¹ì‚¬ì´íŠ¸ë¥¼ ë°©ë¬¸í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ì‹¤ì‹œê°„ ìƒë‹´ì›ê³¼ í†µí™”í•˜ì‹œë ¤ë©´ ì›”ìš”ì¼ë¶€í„° ê¸ˆìš”ì¼ê¹Œì§€ ë™ë¶€ ì‹œê°„ ê¸°ì¤€ ì˜¤ì „ 8:30ë¶€í„° ì˜¤í›„ 6:00ê¹Œì§€ 1-800-761-DOVE (3683)ë¡œ ì „í™”í•´ ì£¼ì‹­ì‹œì˜¤. ì˜ë£Œ ë˜ëŠ” ì œí’ˆ ì•ˆì „ ë¹„ìƒ ìƒí™©ì¸ ê²½ìš°, 1-800-297-7421ë¡œ ì „í™”í•´ ì£¼ì‹­ì‹œì˜¤.\\n\\npassage 2: DoveÂ® ì›¹ì‚¬ì´íŠ¸ë¥¼ ë°©ë¬¸í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤. ì‹¤ì‹œê°„ ìƒë‹´ì›ê³¼ í†µí™”í•˜ì‹œë ¤ë©´ ì›”ìš”ì¼ë¶€í„° ê¸ˆìš”ì¼ê¹Œì§€ ë™ë¶€ ì‹œê°„ ê¸°ì¤€ ì˜¤ì „ 8:30ë¶€í„° ì˜¤í›„ 6:00ê¹Œì§€ 1-800-761-DOVE (3683)ë¡œ ì „í™”í•´ ì£¼ì‹­ì‹œì˜¤. ì˜ë£Œ ë˜ëŠ” ì œí’ˆ ì•ˆì „ ë¹„ìƒ ìƒí™©ì¸ ê²½ìš°, 1-800-297-7421ë¡œ ì „í™”í•´ ì£¼ì‹­ì‹œì˜¤. ì´ ë²ˆí˜¸ëŠ” í•˜ë£¨ 24ì‹œê°„, ì£¼ 7ì¼ ì´ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.\\n\\npassage 3: ê³µìœ í•˜ê¸°. DoveëŠ” ì•„ë¦„ë‹¤ì›€ì´ë€ ìµœìƒì˜ ìƒíƒœë¥¼ ëŠë¼ê³  ë³´ì´ëŠ” ê²ƒì´ ì ì ˆí•œ ê´€ë¦¬ì˜ ê²°ê³¼ë¼ê³  ë¯¿ìŠµë‹ˆë‹¤. DoveëŠ” í•­ìƒ í”¼ë¶€ë‚˜ ë¨¸ë¦¬ì¹´ë½ì˜ ìƒíƒœë¥¼ ëˆˆì— ë„ê²Œ ê°œì„ í•˜ê³ , ê´€ë¦¬ì˜ ì¦ê±°ìš´ ê²½í—˜ì„ ì œê³µí•˜ëŠ” ì œí’ˆì„ ì œê³µí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤. ì™œëƒí•˜ë©´ ì•„ë¦„ë‹µê²Œ ë³´ì´ê³  ëŠë‚„ ë•Œ ë” í–‰ë³µí•´ì§€ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\\n###ì§ˆë¬¸:\\në„ë¸Œ ë¹„ëˆ„ ë³¸ì‚¬ëŠ” ì–´ë””ì— ìˆë‚˜ìš”?\\n###ë‹µ:\\n\\nì£¼ì–´ì§„ ë¬¸êµ¬ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë„ë¸Œ ë¹„ëˆ„ ë³¸ì‚¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤:\\n\\n* ë„ë¸Œ ë¹„ëˆ„ ë³¸ì‚¬ëŠ” ë„ë¸Œ ì›¹ì‚¬ì´íŠ¸ë¥¼ ë°©ë¬¸í•´ ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤ (ë¬¸êµ¬ 1). ë„ë¸Œ ì›¹ì‚¬ì´íŠ¸ë¥¼ ë°©ë¬¸í•˜ì—¬ ë„ë¸Œ ë¹„ëˆ„ì˜ ì œí’ˆê³¼ ì„œë¹„ìŠ¤ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ìœ¼ì„¸ìš”.\\n* ë„ë¸Œ ë¹„ëˆ„ ì‹¤ì‹œê°„ ìƒë‹´ì›ì€ ì›”ìš”ì¼ë¶€í„° ê¸ˆìš”ì¼ê¹Œì§€ ë™ë¶€ ì‹œê°„ ê¸°ì¤€ ì˜¤ì „ 8:30ë¶€í„° ì˜¤í›„ 6:00ê¹Œì§€ 1-800-761-DOVE(3683)ë¡œ ì „í™”í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ë¬¸êµ¬ 1). ë„ë¸Œ ë¹„ëˆ„ ì‹¤ì‹œê°„ ìƒë‹´ì›ì€ ë„ë¸Œ ë¹„ëˆ„ì˜ ì œí’ˆê³¼ ì„œë¹„ìŠ¤ì— ëŒ€í•œ ì§ˆë¬¸ì´ë‚˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n* ë„ë¸Œ ë¹„ëˆ„ì˜ ì œí’ˆ ì•ˆì „ ë¹„ìƒ ìƒí™©ì€ 1-800-297-7421ë¡œ ì „í™”í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ë¬¸êµ¬ 2). ë„ë¸Œ ë¹„ëˆ„ì˜ ì œí’ˆ ì•ˆì „ ë¹„ìƒ ìƒí™©ì€ ë„ë¸Œ ë¹„ëˆ„ì˜ ì œí’ˆì— ëŒ€í•œ ì§ˆë¬¸ì´ë‚˜ ë¬¸ì œë¥¼ í•´ê²°í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n* ë„ë¸Œ ë¹„ëˆ„ì˜ ì œí’ˆì€ ì•„ë¦„ë‹¤ì›€ì„ í–¥ìƒì‹œí‚¤ê³  í”¼ë¶€ë‚˜ ë¨¸ë¦¬ì¹´ë½ì˜ ìƒíƒœë¥¼ ê°œì„ í•˜ëŠ” ì œí’ˆì„ ì œê³µí•©ë‹ˆë‹¤ (ë¬¸êµ¬ 3). ë„ë¸Œ ë¹„ëˆ„ì˜ ì œí’ˆì€ ë„ë¸Œ ë¹„ëˆ„ì˜ ì œí’ˆê³¼ ì„œë¹„ìŠ¤ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ìœ¼ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n\\në”°ë¼ì„œ, ë„ë¸Œ ë¹„ëˆ„ ë³¸ì‚¬ëŠ” ë¬¸êµ¬ 1ê³¼ ë¬¸êµ¬ 2ì—ì„œ ì–¸ê¸‰ëœ ë„ë¸Œ ë¹„ëˆ„ ì›¹ì‚¬ì´íŠ¸ ë˜ëŠ” ë„ë¸Œ ë¹„ëˆ„ ì‹¤ì‹œê°„ ìƒë‹´ì›ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b51211-7129-49b1-9461-6944e0a81196",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
