{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741a454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade pip\n",
    "!pip install trl wandb\n",
    "!pip install -U bitsandbytes\n",
    "!pip install \"unsloth[cu121-ampere-torch220] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788bf1a0-cb7e-458d-8549-a463ba230044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset, Dataset\n",
    "import huggingface_hub\n",
    "from transformers import (\n",
    "    pipeline,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "from trl.core import LengthSampler\n",
    "from trl import (\n",
    "    PPOTrainer,\n",
    "    PPOConfig,\n",
    "    AutoModelForCausalLMWithValueHead,\n",
    "    create_reference_model,\n",
    "    DPOConfig,\n",
    "    DPOTrainer,\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    AutoPeftModel,\n",
    "    AutoPeftModelForCausalLM,\n",
    "    PeftModel,\n",
    "    LoraConfig,\n",
    "    LoftQConfig,\n",
    "    TaskType,\n",
    ")\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "# from unsloth import is_bfloat16_supported\n",
    "# from unsloth import FastLanguageModel\n",
    "# from unsloth.chat_templates import (\n",
    "#     get_chat_template,\n",
    "#     train_on_responses_only,\n",
    "#     standardize_sharegpt,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a53a8ca",
   "metadata": {},
   "source": [
    "### Model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658d9036",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44cd4a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.12.8: Fast Llama patching. Transformers: 4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA A100 80GB PCIe. Max memory: 79.254 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.0+cu124. CUDA: 8.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post2. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c4b5f2529549b392a87f20bef557a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.24G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69d1807c6cf4492fa3bf076efa0288ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24122bdfdc51402eaaf83a6f334e8100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/54.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "722e1571d7ce451b9e7f3290439f9f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "181a1112c7034591895a623353941f8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/454 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "model_id = 'Bllossom/llama-3.2-Korean-Bllossom-3B'\n",
    "model_id = 'qa_kor_v11'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548079b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ce6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04211b93",
   "metadata": {},
   "source": [
    "### Dataset load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71261c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('HoJL/law_expc')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3ebc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"\"\"너는 주어지는 내용만을 보고 질문에 답을 하고 왜 이런 답을 했는지 추론도 해주는 역할이야. \n",
    "반드시 한국어로 답변해줘.\n",
    "답변은 아래형식과 같이 추론과 답으로 이루어져 있고 마크다운으로 내줘.\n",
    "\n",
    "### 추론:\n",
    "### 답:\n",
    "\"\"\"\n",
    "inputs = f\"\"\"\n",
    "### 내용:\n",
    "{dataset['test'][2].get('관계법령_정리')}\n",
    "\n",
    "### 질문:\n",
    "{dataset['test'][2].get('질의요지')}\n",
    "\"\"\"\n",
    "messages = [\n",
    "{\"role\": \"system\", \"content\": f\"{instruction}\"},\n",
    "{\"role\": \"user\", \"content\": f\"{inputs}\"}\n",
    "]\n",
    "input = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c661359",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id = tokenizer.eos_token_id \n",
    "inputs = tokenizer(\n",
    "    input,\n",
    "    return_tensors=\"pt\",\n",
    ").to(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb45ee31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "terminators = [\n",
    "    tokenizer.convert_tokens_to_ids(\"<|end_of_text|>\"),\n",
    "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "#skip_special_tokens=True,\n",
    "text_streamer = TextStreamer(tokenizer,  skip_prompt=True)\n",
    "_ = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=4096,  # 최대 생성 토큰 수를 설정합니다.\n",
    "    eos_token_id=terminators,  # 생성을 멈출 기준을 설정합니다.\n",
    "    temperature=0.1,\n",
    "    repetition_penalty=1.1,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
